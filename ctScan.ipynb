{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Untitled0.ipynb_\n",
    "\n",
    "Hey there!\n",
    "\n",
    "I wanted to share this awesome notebook I've been working on. It's all about processing medical imaging data and getting it ready for some serious machine learning action. Let me walk you through what's going on here: What's Inside\n",
    "\n",
    "First off, we've got a bunch of libraries that are super helpful for different parts of the process:\n",
    "\n",
    "os, zipfile, glob: These are for handling files and directories. Think of them as our handy tools for organizing and accessing data.\n",
    "\n",
    "pydicom: This one's a lifesaver for reading DICOM files, which are super common in medical imaging.\n",
    "\n",
    "numpy, pandas: These are our go-to libraries for crunching numbers and managing data. They make it easy to work with large datasets.\n",
    "\n",
    "xml.etree.ElementTree: This helps us deal with XML data, which often comes with medical images to store important metadata.\n",
    "\n",
    "tensorflow, keras: These are the brains behind our deep learning models. They help us build and train neural networks.\n",
    "\n",
    "h5py: This is great for handling HDF5 files, which are perfect for storing large amounts of data efficiently.\n",
    "\n",
    "json: We use this for parsing JSON data, which is handy for configuration files and data exchange.\n",
    "\n",
    "multiprocessing: This lets us speed things up by using multiple CPU cores. It's like having a team of helpers working together to get the job done faster.\n",
    "\n",
    "gc: This helps us manage memory usage, which is crucial when dealing with big datasets.\n",
    "\n",
    "logging: This is for keeping track of what's happening in our code. It's like a journal that helps us debug and understand the process better.\n",
    "\n",
    "The Game Plan\n",
    "\n",
    "Here's a quick rundown of what we're doing in this notebook:\n",
    "\n",
    "Data Extraction: We start by extracting medical imaging data from ZIP files and reading DICOM files.\n",
    "\n",
    "Data Preprocessing: Next, we convert the raw data into a format that's ready for machine learning. This might involve resizing images, normalizing data, and other tweaks.\n",
    "\n",
    "Metadata Handling: We parse XML files to grab important metadata and store it in a structured format using pandas.\n",
    "\n",
    "Data Augmentation: We use tensorflow and keras to create more diverse training data, which can help our models perform better.\n",
    "\n",
    "Model Training: We define and train a deep learning model. Once it's trained, we can save it in HDF5 format.\n",
    "\n",
    "Parallel Processing: We use multiprocessing to speed up data preprocessing and model training by splitting the work across multiple CPU cores.\n",
    "\n",
    "Memory Management: We keep an eye on memory usage with gc to make sure we don't run into any issues.\n",
    "\n",
    "Logging: We log important events and errors to keep track of what's happening and debug any problems.\n",
    "\n",
    "Why It's Cool\n",
    "\n",
    "This notebook is a great template for processing medical imaging data and training deep learning models. You can easily adapt it to different datasets and models by tweaking the data preprocessing and model training sections. Plus, with parallel processing and memory management, it can handle even the biggest datasets efficiently. Wrap-Up\n",
    "\n",
    "By following this pipeline, you can turn raw medical imaging data into powerful machine learning models that can do things like classify images, segment structures, and detect anomalies. It's a pretty awesome way to leverage the power of deep learning in the medical field.\n",
    "\n",
    "Hope you find this helpful! Let me know if you have any questions or need a hand with anything.\n",
    "\n",
    "Cheers!\n",
    "\n",
    "!pip install pydicom tqdm h5py\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "Colab paid products - Cancel contracts here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Importing Libraries\n",
    "This section imports all the necessary libraries for the script. These libraries cover a range of functionalities, including file handling, data processing, XML parsing, machine learning, and parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import glob\n",
    "import pydicom\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "import h5py\n",
    "import json\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import gc\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Setting Up Logging\n",
    "This line sets up basic logging configuration. It specifies that log messages should include the timestamp, log level, and the message itself. This is useful for tracking the progress and debugging issues during execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Defining the MedicalImagePreprocessor Class\n",
    "This class is the core of the preprocessing pipeline. It initializes with paths for the input ZIP file, metadata file, and output file. It also defines directories and files for extracted data and annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedicalImagePreprocessor:\n",
    "    def __init__(self, zipPath, metadataPath, outputPath):\n",
    "        self.zipPath = zipPath\n",
    "        self.metadataPath = metadataPath\n",
    "        self.outputPath = outputPath\n",
    "        self.extractedDir = '/content/extractedData'\n",
    "        self.annotationsFile = 'annotations.json'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Extracting ZIP Files\n",
    "This method extracts the contents of the ZIP file into a specified directory. It ensures the directory exists before extracting the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractZipFiles(self):\n",
    "        os.makedirs(self.extractedDir, exist_ok=True)\n",
    "        with zipfile.ZipFile(self.zipPath, 'r') as zipRef:\n",
    "            zipRef.extractall(self.extractedDir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Parsing XML Files\n",
    "This method parses an XML file to extract the study UID and malignancy scores. It handles namespaces and searches for specific elements within the XML structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseXmlFile(self, xmlPath):\n",
    "        try:\n",
    "            tree = ET.parse(xmlPath)\n",
    "            rootXml = tree.getroot()\n",
    "            namespaces = {'nih': 'http://www.nih.gov'}\n",
    "\n",
    "            studyUidElement = rootXml.find('.//nih:StudyInstanceUID', namespaces)\n",
    "            if studyUidElement is None:\n",
    "                studyUidElement = rootXml.find('.//nih:CXRSeriesInstanceUid', namespaces)\n",
    "\n",
    "            studyUid = studyUidElement.text if studyUidElement is not None else None\n",
    "\n",
    "            malignancyScores = [int(nodule.find('.//nih:malignancy', namespaces).text)\n",
    "                                for nodule in rootXml.findall('.//nih:unblindedReadNodule', namespaces)\n",
    "                                if nodule.find('.//nih:malignancy', namespaces) is not None]\n",
    "\n",
    "            return studyUid, max(malignancyScores) if malignancyScores else 0\n",
    "        except Exception:\n",
    "            return None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Parsing XML Annotations\n",
    "This method processes all XML files in a directory to extract annotations. It uses parallel processing to speed up the operation and saves the results to a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseXmlAnnotations(self):\n",
    "        lidcDir = os.path.join(self.extractedDir, 'LIDC-IDRI')\n",
    "        xmlFiles = glob.glob(os.path.join(lidcDir, '**/*.xml'), recursive=True)\n",
    "\n",
    "        with Pool(max(1, cpu_count() - 1)) as pool:\n",
    "            results = pool.map(self.parseXmlFile, xmlFiles)\n",
    "\n",
    "        annotations = {studyUid: malignancy for studyUid, malignancy in results if studyUid}\n",
    "\n",
    "        with open(self.annotationsFile, \"w\") as f:\n",
    "            json.dump(annotations, f)\n",
    "\n",
    "        return annotations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7: Loading Annotations\n",
    "This method loads annotations from a JSON file if it exists. It returns an empty dictionary if the file is not found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadAnnotations(self):\n",
    "        if os.path.exists(self.annotationsFile):\n",
    "            with open(self.annotationsFile, \"r\") as f:\n",
    "                return json.load(f)\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7: Loading Annotations\n",
    "This method loads annotations from a JSON file if it exists. It returns an empty dictionary if the file is not found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadAnnotations(self):\n",
    "        if os.path.exists(self.annotationsFile):\n",
    "            with open(self.annotationsFile, \"r\") as f:\n",
    "                return json.load(f)\n",
    "        return {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 8: Processing DICOM Files\n",
    "This method processes a DICOM file to extract and normalize the image data. It resizes the image and assigns a label based on the annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processDicomFile(self, dicomPath, studyUid, annotations):\n",
    "        try:\n",
    "            ds = pydicom.dcmread(dicomPath)\n",
    "            image = ds.pixel_array.astype(np.float32)\n",
    "\n",
    "            image = (image - np.min(image)) / (np.max(image) - np.min(image))\n",
    "            image = np.array(tf.image.resize(image[..., np.newaxis], (128, 128)))\n",
    "\n",
    "            label = 1 if annotations.get(studyUid, 0) > 3 else 0\n",
    "            return image, label\n",
    "        except Exception:\n",
    "            return None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 9: Preprocessing Images\n",
    "This method preprocesses all the images in the dataset. It reads the metadata, processes DICOM files, and saves the preprocessed data to an HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessImages(self):\n",
    "        metadataDf = pd.read_csv(self.metadataPath)\n",
    "        ctSeries = metadataDf[metadataDf['Modality'] == 'CT']\n",
    "\n",
    "        annotations = self.loadAnnotations()\n",
    "        if not annotations:\n",
    "            annotations = self.parseXmlAnnotations()\n",
    "\n",
    "        slices, labels, studyUids = [], [], []\n",
    "\n",
    "        for _, row in ctSeries.iterrows():\n",
    "            studyUid = row['Study UID']\n",
    "            fileLocation = row['File Location'].lstrip('.\\\\')\n",
    "            patientDir = os.path.join(self.extractedDir, fileLocation)\n",
    "\n",
    "            if os.path.exists(patientDir):\n",
    "                dicomFiles = glob.glob(os.path.join(patientDir, \"*.dcm\"))\n",
    "\n",
    "                with Pool(max(1, cpu_count() // 2)) as pool:\n",
    "                    results = pool.starmap(self.processDicomFile,\n",
    "                                           [(dicomFile, studyUid, annotations) for dicomFile in dicomFiles])\n",
    "\n",
    "                for image, label in results:\n",
    "                    if image is not None and label is not None:\n",
    "                        slices.append(image)\n",
    "                        labels.append(label)\n",
    "                        studyUids.append(studyUid)\n",
    "\n",
    "        slices = np.array(slices)\n",
    "        labels = np.array(labels)\n",
    "\n",
    "        os.makedirs(os.path.dirname(self.outputPath), exist_ok=True)\n",
    "        with h5py.File(self.outputPath, 'w') as hdf:\n",
    "            hdf.create_dataset('slices', data=slices, compression='gzip', chunks=True)\n",
    "            hdf.create_dataset('labels', data=labels, compression='gzip')\n",
    "            hdf.create_dataset('studyUids', data=np.array(studyUids, dtype='S'), compression='gzip')\n",
    "\n",
    "        return slices, labels, studyUids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 10: Cleaning Up\n",
    "This method cleans up the extracted data and annotations file to free up space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(self):\n",
    "        os.system(f'rm -rf {self.extractedDir}')\n",
    "        os.system(f'rm {self.annotationsFile}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
